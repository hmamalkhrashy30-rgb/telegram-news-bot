import os
import logging
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
from flask import Flask, jsonify
import threading
import time
import random
import hashlib
from datetime import datetime
from fake_useragent import UserAgent

# ========== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ==========
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
CHANNEL_USERNAME = os.getenv('CHANNEL_USERNAME', '@DO_IUi')
CHECK_INTERVAL = int(os.getenv('CHECK_INTERVAL', '1800'))  # 30 Ø¯Ù‚ÙŠÙ‚Ø©

if not TELEGRAM_BOT_TOKEN:
    raise ValueError("âŒ TELEGRAM_BOT_TOKEN ØºÙŠØ± Ù…Ø­Ø¯Ø¯.")

# ========== Flask App ==========
app = Flask(__name__)

# ========== Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© ==========
NEWS_SOURCES = [
    # Ù…ØµØ§Ø¯Ø± Investing.com (Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)
    ("Ø£Ø®Ø¨Ø§Ø± Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©", "https://www.investing.com/", "economy"),
    ("Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ù…Ø§Ù„", "https://www.investing.com/markets/", "markets"),
    ("Ø³Ù„Ø¹", "https://www.investing.com/commodities/", "commodities"),
    
    # Ù…ØµØ§Ø¯Ø± Ø¨Ø¯ÙŠÙ„Ø©
    ("Ø±ÙˆÙŠØªØ±Ø² Ø§Ù‚ØªØµØ§Ø¯", "https://www.reuters.com/business/", "reuters"),
    ("Ø¨Ù„ÙˆÙ…Ø¨Ø±Ø¬", "https://www.bloomberg.com/markets", "bloomberg"),
    ("CNN Ø£Ø¹Ù…Ø§Ù„", "https://edition.cnn.com/business", "cnn"),
    
    # Ù…ØµØ§Ø¯Ø± RSS Ù…Ø¨Ø§Ø´Ø±Ø© (Ø£Ø³Ù‡Ù„ ÙˆØ£Ø³Ø±Ø¹)
    ("Investing RSS", "https://www.investing.com/rss/news.rss", "rss"),
    ("Reuters RSS", "https://www.reutersagency.com/feed/?best-topics=business-finance&post_type=best", "rss"),
]

# Ù…ØµØ§Ø¯Ø± RSS Ù…Ø¨Ø§Ø´Ø±Ø©
RSS_FEEDS = [
    ("Ø£Ø®Ø¨Ø§Ø± Investing", "https://www.investing.com/rss/news_285.rss"),
    ("Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø£Ø³ÙˆØ§Ù‚", "https://www.investing.com/rss/news_25.rss"),
    ("Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù†ÙØ·", "https://www.investing.com/rss/news_3.rss"),
    ("Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø°Ù‡Ø¨", "https://www.investing.com/rss/news_4.rss"),
    ("Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Øª", "https://www.investing.com/rss/news_2.rss"),
]

KEYWORDS = {
    'ÙØ§Ø¦Ø¯Ø©': ['interest rate', 'fed', 'central bank', 'ÙØ§Ø¦Ø¯Ø©', 'Ø¨Ù†Ùƒ Ù…Ø±ÙƒØ²ÙŠ', 'Ø§Ù„ÙÙŠØ¯Ø±Ø§Ù„ÙŠ'],
    'ØªØ¶Ø®Ù…': ['cpi', 'inflation', 'ØªØ¶Ø®Ù…', 'Ø£Ø³Ø¹Ø§Ø±', 'inflation'],
    'Ø¨Ø·Ø§Ù„Ø©': ['unemployment', 'jobs', 'Ø¨Ø·Ø§Ù„Ø©', 'ÙˆØ¸Ø§Ø¦Ù', 'employment'],
    'Ù†Ø§ØªØ¬': ['gdp', 'growth', 'Ù†Ø§ØªØ¬', 'Ø§Ù‚ØªØµØ§Ø¯', 'Ø§Ù‚ØªØµØ§Ø¯ÙŠ'],
    'Ù†ÙØ·': ['oil', 'crude', 'Ø¨ØªØ±ÙˆÙ„', 'Ù†ÙØ·', 'Ø£ÙˆØ¨Ùƒ', 'Ø§Ù„Ù†ÙØ·'],
    'Ø°Ù‡Ø¨': ['gold', 'Ø°Ù‡Ø¨', 'Ù…Ø¹Ø¯Ù†', 'Ø§Ù„Ø°Ù‡Ø¨', 'bullion'],
    'Ø­Ø±Ø¨': ['war', 'conflict', 'Ø­Ø±Ø¨', 'ØµØ±Ø§Ø¹', 'Ù†Ø²Ø§Ø¹'],
    'Ø¹Ù‚ÙˆØ¨Ø§Øª': ['sanctions', 'Ø¹Ù‚ÙˆØ¨Ø§Øª', 'Ø¹Ù‚ÙˆØ¨Ø©', 'embargo'],
}

# ØªØ®Ø²ÙŠÙ†
sent_articles = set()
bot_started = False
last_check_time = None
ua = UserAgent()

@app.route('/')
def home():
    return jsonify({
        "status": "running",
        "service": "Telegram News Bot",
        "channel": CHANNEL_USERNAME,
        "bot_started": bot_started,
        "articles_in_memory": len(sent_articles),
        "last_check": last_check_time,
        "uptime": time.strftime("%H:%M:%S", time.gmtime(time.time() - start_time))
    })

@app.route('/health')
def health():
    return jsonify({"status": "healthy", "time": datetime.now().isoformat()}), 200

@app.route('/check')
def check_now():
    """ÙØ­Øµ ÙŠØ¯ÙˆÙŠ"""
    threading.Thread(target=run_manual_check).start()
    return jsonify({"message": "Ø¨Ø¯Ø£ Ø§Ù„ÙØ­Øµ Ø§Ù„ÙŠØ¯ÙˆÙŠ", "time": datetime.now().strftime("%H:%M:%S")})

@app.route('/test/<path:url>')
def test_url(url):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù„Ø¨ URL Ù…Ø¹ÙŠÙ†"""
    async def test():
        async with aiohttp.ClientSession() as session:
            headers = await get_headers()
            try:
                async with session.get(f"https://{url}", headers=headers, timeout=10) as resp:
                    return jsonify({
                        "url": url,
                        "status": resp.status,
                        "headers": dict(resp.headers)
                    })
            except Exception as e:
                return jsonify({"error": str(e)}), 500
    
    return asyncio.run(test())

# ========== ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø© ==========
async def get_headers():
    """Ø¥Ù†Ø´Ø§Ø¡ headers Ù…Ø­Ø§ÙƒÙŠØ© Ù„Ù„Ù…ØªØµÙØ­"""
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,ar;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.google.com/',
    }

def get_proxy():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ proxy Ù…Ø¬Ø§Ù†ÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)"""
    proxies = [
        None,  # Ø¨Ø¯ÙˆÙ† proxy Ø£ÙˆÙ„Ø§Ù‹
        'http://proxy1:8080',
        'http://proxy2:8080',
    ]
    return random.choice(proxies)

def create_id(title, source):
    """Ø¥Ù†Ø´Ø§Ø¡ ID ÙØ±ÙŠØ¯ Ù„Ù„Ø®Ø¨Ø±"""
    text = f"{title[:50]}{source}"
    return hashlib.md5(text.encode()).hexdigest()[:12]

def categorize(title):
    """ØªØµÙ†ÙŠÙ Ø§Ù„Ø®Ø¨Ø±"""
    title_lower = title.lower()
    for cat, keywords in KEYWORDS.items():
        for kw in keywords:
            if kw.lower() in title_lower:
                return cat
    return "Ø¹Ø§Ù…"

# ========== Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS (Ø£ÙØ¶Ù„ ÙˆØ£Ø³Ù‡Ù„) ==========
async def fetch_rss_feed(session, url, source_name):
    """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS feed"""
    try:
        headers = await get_headers()
        async with session.get(url, headers=headers, timeout=15) as response:
            if response.status == 200:
                xml = await response.text()
                return parse_rss_feed(xml, source_name)
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ RSS {source_name}: {e}")
    return []

def parse_rss_feed(xml, source_name):
    """ØªØ­Ù„ÙŠÙ„ RSS feed"""
    try:
        soup = BeautifulSoup(xml, 'xml')
        articles = []
        
        items = soup.find_all('item')[:15]  # Ø£ÙˆÙ„ 15 Ø®Ø¨Ø±
        
        for item in items:
            try:
                title = item.find('title').text.strip()
                link = item.find('link').text.strip()
                pub_date = item.find('pubDate')
                time_text = pub_date.text.strip() if pub_date else "Ù‚Ø¨Ù„ Ù‚Ù„ÙŠÙ„"
                
                # ÙˆØµÙ
                description = item.find('description')
                summary = description.text.strip()[:200] if description else ""
                
                news_type = categorize(title)
                
                article_data = {
                    'id': create_id(title, source_name),
                    'title': title,
                    'link': link,
                    'time': time_text,
                    'summary': summary,
                    'type': news_type,
                    'source': source_name,
                    'timestamp': time.time()
                }
                
                articles.append(article_data)
                
            except Exception:
                continue
        
        logging.info(f"ğŸ“¡ RSS {source_name}: {len(articles)} Ø®Ø¨Ø±")
        return articles
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ RSS: {e}")
        return []

# ========== Ø¬Ù„Ø¨ Ù…Ù† Investing.com (Ù…Ø­Ø§ÙˆÙ„Ø© Ø°ÙƒÙŠØ©) ==========
async def fetch_investing_smart(session, url, category):
    """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø¨Ø°ÙƒØ§Ø¡ Ù…Ù† Investing.com"""
    try:
        headers = await get_headers()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
        async with session.get(url, headers=headers, timeout=20) as response:
            if response.status != 200:
                logging.warning(f"âš ï¸ {category}: Ø­Ø§Ù„Ø© {response.status}")
                return []
            
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            articles = []
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ©
            patterns = [
                # Ø£Ù†Ù…Ø§Ø· Investing.com Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
                {'selector': 'article[data-test="article-item"]', 'title': 'a[data-test="article-title"]'},
                {'selector': 'div.articleItem', 'title': 'a.title'},
                {'selector': 'div.largeTitle', 'title': 'a'},
                {'selector': 'div.mediumTitle', 'title': 'a'},
                {'selector': 'div.textDiv', 'title': 'a'},
                {'selector': '[class*="article"]', 'title': '[class*="title"]'},
            ]
            
            for pattern in patterns:
                items = soup.select(pattern['selector'])[:10]
                if items:
                    for item in items:
                        try:
                            title_elem = item.select_one(pattern['title'])
                            if not title_elem:
                                continue
                            
                            title = title_elem.text.strip()
                            if len(title) < 10:
                                continue
                            
                            link = title_elem.get('href', '')
                            if link and not link.startswith('http'):
                                link = f"https://www.investing.com{link}"
                            
                            time_elem = item.find('time') or item.find('span', class_='date')
                            time_text = time_elem.text.strip() if time_elem else "Ù‚Ø¨Ù„ Ù‚Ù„ÙŠÙ„"
                            
                            news_type = categorize(title)
                            
                            article_data = {
                                'id': create_id(title, category),
                                'title': title,
                                'link': link,
                                'time': time_text,
                                'type': news_type,
                                'source': category,
                                'timestamp': time.time()
                            }
                            
                            articles.append(article_data)
                            
                        except Exception:
                            continue
                    
                    if articles:
                        break
            
            logging.info(f"ğŸ“¡ {category}: ÙˆØ¬Ø¯ {len(articles)} Ø®Ø¨Ø±")
            return articles
            
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {category}: {e}")
        return []

# ========== Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ ØªÙ„ÙŠØ¬Ø±Ø§Ù… ==========
async def send_to_channel(bot, article):
    """Ø¥Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ù†Ø§Ø©"""
    try:
        emoji_map = {
            'ÙØ§Ø¦Ø¯Ø©': 'ğŸ¦', 'ØªØ¶Ø®Ù…': 'ğŸ“ˆ', 'Ø¨Ø·Ø§Ù„Ø©': 'ğŸ‘¥',
            'Ù†Ø§ØªØ¬': 'ğŸ“Š', 'Ù†ÙØ·': 'ğŸ›¢ï¸', 'Ø°Ù‡Ø¨': 'ğŸ’°',
            'Ø­Ø±Ø¨': 'âš”ï¸', 'Ø¹Ù‚ÙˆØ¨Ø§Øª': 'ğŸš«', 'Ø¹Ø§Ù…': 'ğŸ“°'
        }
        
        emoji = emoji_map.get(article['type'], 'ğŸ“°')
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        message = f"""
{emoji} **{article['type'].upper()}** | {article['source']}

{article['title']}

â° {article['time']}

ğŸ”— [Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø®Ø¨Ø±]({article['link']})
        """
        
        await bot.send_message(
            chat_id=CHANNEL_USERNAME,
            text=message[:4000],
            parse_mode='Markdown',
            disable_web_page_preview=False
        )
        
        logging.info(f"âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„: {article['title'][:60]}...")
        sent_articles.add(article['id'])
        return True
        
    except Exception as e:
        logging.error(f"âŒ ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„: {str(e)[:100]}")
        return False

# ========== Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ==========
async def news_bot_loop():
    """Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø¨ÙˆØª"""
    global bot_started, last_check_time
    
    try:
        bot = Bot(token=TELEGRAM_BOT_TOKEN)
        bot_info = await bot.get_me()
        logging.info(f"ğŸ¤– Ø§Ù„Ø¨ÙˆØª Ø¬Ø§Ù‡Ø²: @{bot_info.username}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
        try:
            await bot.send_message(
                chat_id=CHANNEL_USERNAME,
                text="ğŸš€ Ø¨ÙˆØª Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù†!\nØ³ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹."
            )
        except:
            pass
        
        bot_started = True
        
    except Exception as e:
        logging.error(f"âŒ ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª: {e}")
        return
    
    async with aiohttp.ClientSession() as session:
        while True:
            try:
                last_check_time = datetime.now().strftime("%H:%M:%S")
                logging.info("=" * 60)
                logging.info(f"ğŸ”„ Ø¨Ø¯Ø¡ ÙØ­Øµ Ø¬Ø¯ÙŠØ¯: {last_check_time}")
                
                all_articles = []
                
                # 1. Ø£ÙˆÙ„Ø§Ù‹: Ø¬Ù„Ø¨ Ù…Ù† RSS (Ø§Ù„Ø£Ø³Ù‡Ù„ ÙˆØ§Ù„Ø£ÙƒØ«Ø± Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©)
                logging.info("ğŸ“¡ Ù…Ø±Ø­Ù„Ø© 1: Ø¬Ù„Ø¨ Ù…Ù† RSS feeds...")
                for source_name, rss_url in RSS_FEEDS:
                    articles = await fetch_rss_feed(session, rss_url, source_name)
                    all_articles.extend(articles)
                    await asyncio.sleep(1)
                
                # 2. Ø«Ø§Ù†ÙŠØ§Ù‹: Ø¬Ù„Ø¨ Ù…Ù† Investing.com (Ø¥Ø°Ø§ ÙƒØ§Ù† RSS Ù‚Ù„ÙŠÙ„Ø§Ù‹)
                if len(all_articles) < 5:
                    logging.info("ğŸ“¡ Ù…Ø±Ø­Ù„Ø© 2: Ø¬Ù„Ø¨ Ù…Ù† Investing.com...")
                    for category, url, _ in NEWS_SOURCES[:3]:  # Ø£ÙˆÙ„ 3 Ù…ØµØ§Ø¯Ø± ÙÙ‚Ø·
                        articles = await fetch_investing_smart(session, url, category)
                        all_articles.extend(articles)
                        await asyncio.sleep(2)
                
                # 3. ØªØµÙÙŠØ© ÙˆØªØ±ØªÙŠØ¨
                important_articles = [a for a in all_articles if a['type'] != 'Ø¹Ø§Ù…']
                important_articles.sort(key=lambda x: x['timestamp'], reverse=True)
                
                # 4. Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                sent_count = 0
                for article in important_articles[:8]:  # Ø£ÙˆÙ„ 8 Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
                    if article['id'] not in sent_articles:
                        success = await send_to_channel(bot, article)
                        if success:
                            sent_count += 1
                            await asyncio.sleep(2)
                
                # 5. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                logging.info("=" * 60)
                logging.info(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
                logging.info(f"   ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {len(all_articles)}")
                logging.info(f"   â­ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ù‡Ù…Ø©: {len(important_articles)}")
                logging.info(f"   ğŸ“¤ Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ø­Ø¯ÙŠØ«Ø§Ù‹: {sent_count}")
                logging.info(f"   ğŸ’¾ Ø§Ù„Ù…Ø®Ø²Ù†Ø©: {len(sent_articles)}")
                
                if len(all_articles) == 0:
                    logging.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø±! Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„...")
                    test_resp = await session.get('https://www.google.com', headers=await get_headers())
                    logging.info(f"ğŸŒ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„: {test_resp.status}")
                
                logging.info("=" * 60)
                
            except Exception as e:
                logging.error(f"ğŸš¨ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¯ÙˆØ±Ø©: {e}")
            
            logging.info(f"â³ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± {CHECK_INTERVAL//60} Ø¯Ù‚Ø§Ø¦Ù‚ Ù„Ù„ÙØ­Øµ Ø§Ù„ØªØ§Ù„ÙŠ...")
            await asyncio.sleep(CHECK_INTERVAL)

def run_manual_check():
    """ÙØ­Øµ ÙŠØ¯ÙˆÙŠ"""
    async def check():
        try:
            bot = Bot(token=TELEGRAM_BOT_TOKEN)
            async with aiohttp.ClientSession() as session:
                logging.info("ğŸ” ÙØ­Øµ ÙŠØ¯ÙˆÙŠ Ø³Ø±ÙŠØ¹...")
                
                # Ø§Ø®ØªØ¨Ø§Ø± RSS Ù…Ø¨Ø§Ø´Ø±Ø©
                articles = []
                for source_name, rss_url in RSS_FEEDS[:2]:  # Ø£ÙˆÙ„ Ù…ØµØ¯Ø±ÙŠÙ† ÙÙ‚Ø·
                    feed_articles = await fetch_rss_feed(session, rss_url, source_name)
                    articles.extend(feed_articles)
                
                if articles:
                    logging.info(f"âœ… Ø§Ù„ÙØ­Øµ Ø§Ù„ÙŠØ¯ÙˆÙŠ: {len(articles)} Ø®Ø¨Ø±")
                    for article in articles[:3]:  # Ø£ÙˆÙ„ 3
                        await send_to_channel(bot, article)
                        await asyncio.sleep(1)
                else:
                    logging.warning("âš ï¸ Ø§Ù„ÙØ­Øµ Ø§Ù„ÙŠØ¯ÙˆÙŠ: 0 Ø®Ø¨Ø±")
                    
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙØ­Øµ Ø§Ù„ÙŠØ¯ÙˆÙŠ: {e}")
    
    asyncio.run(check())

def start_bot():
    """Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(news_bot_loop())

def run_flask():
    """ØªØ´ØºÙŠÙ„ Flask"""
    port = int(os.getenv('PORT', 10000))
    app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)

# ========== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==========
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    start_time = time.time()
    
    logging.info("=" * 70)
    logging.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    logging.info(f"ğŸ“¢ Ø§Ù„Ù‚Ù†Ø§Ø©: {CHANNEL_USERNAME}")
    logging.info(f"â° ÙØªØ±Ø© Ø§Ù„ÙØ­Øµ: {CHECK_INTERVAL} Ø«Ø§Ù†ÙŠØ© ({CHECK_INTERVAL//60} Ø¯Ù‚ÙŠÙ‚Ø©)")
    logging.info(f"ğŸ“¡ Ù…ØµØ§Ø¯Ø± RSS: {len(RSS_FEEDS)}")
    logging.info("=" * 70)
    
    # Ø¨Ø¯Ø¡ Flask ÙÙŠ thread
    flask_thread = threading.Thread(target=run_flask, daemon=True)
    flask_thread.start()
    
    # ØªØ£Ø®ÙŠØ± Ø«Ù… Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª
    time.sleep(5)
    
    # Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª ÙÙŠ thread Ù…Ù†ÙØµÙ„
    bot_thread = threading.Thread(target=start_bot, daemon=True)
    bot_thread.start()
    
    # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ¹Ù…Ù„
    try:
        while True:
            time.sleep(3600)
    except KeyboardInterrupt:
        logging.info("ğŸ‘‹ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨ÙˆØª...")
